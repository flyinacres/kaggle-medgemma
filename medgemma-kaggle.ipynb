{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14657636,"sourceType":"datasetVersion","datasetId":9277098}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"From the associated README:","metadata":{}},{"cell_type":"markdown","source":"# Medical Document Summarizer\n\n**A patient-centered tool that translates complex medical documentation into clear, understandable summaries using Google's MedGemma model.**\n\nThis project addresses the critical gap between the medical information patients receive and their ability to comprehend it. Built for the HAI-DEF Kaggle hackathon, it leverages state-of-the-art medical language models to help patients understand doctors' notes, test results, discharge instructions, and specialist reports without providing medical advice or diagnosis.\n\n## The Problem\n\nResearch shows that only 12% of American adults have the health literacy needed to fully understand what their doctors tell them. Patients forget 40-80% of medical information immediately after consultations, and 78% of emergency department patients leave with deficient comprehension of their discharge instructions‚Äîoften without realizing it. This comprehension gap costs the US healthcare system $106-238 billion annually and is independently associated with a 75% increase in mortality risk among elderly patients.\n\n## The Solution\n\nThis application provides three input modes (text entry, voice recording with medical transcription, and image upload) that feed into MedGemma's medical language understanding. The output is a structured, patient-friendly summary with key takeaways, medication explanations, medical term definitions, and suggested follow-up questions. A built-in chat interface allows patients to ask clarifying questions, all grounded in the original medical documentation.\n\nThe system runs locally on consumer hardware (preserving patient privacy) and is optimized for mobile viewing, as patients often need to reference medical information away from their computers.\n\n## Sources\n\nHealth Literacy:\n\n12% proficient health literacy: 2003 National Assessment of Adult Literacy (NAAL) - NCES Publication 2006-483; HHS/AHRQ 2008 Issue Brief\n\nComprehension Failures:\n\n40-80% forgotten immediately: Kessels, Journal of the Royal Society of Medicine, 2003; AAFP, 2018\n78% ED patients deficient comprehension: Engel et al., Annals of Emergency Medicine, 2009\n\nEconomic Impact:\n\n$106-238 billion annually: Vernon et al., University of Connecticut/National Patient Safety Foundation, 2007\n\nMortality:\n\n75% increased mortality risk (HR 1.75): Sudore et al., Journal of General Internal Medicine, 2006\n\nPrivacy Concerns:\n\n92% consider privacy a right: AMA/Savvy Cooperative survey, AMA 2022\n14% trust tech companies: Rock Health 2023 consumer survey","metadata":{}},{"cell_type":"markdown","source":"# Usage of this notebook\n\nConsult the README, kaggle_setup, and requirements.txt for more information that may be necessary to get this project running. The models require particular versions of libraries and will not run without compatible versions. And as new models and libaries release it will be necessary to adjust the versions appropriately.\n\n## Before running this notebook\nYou must create a prompt directory and populate it with json_prompt.txt and conversational_prompt.txt. THIS DIRECTORY MAY VARY FROM THE CODE: Search for PROMPT_FILE_PATH and ensure it matches your path.\n\nYou must create a valid Huggingface token for the Google MedGemma model access, and add it to this project.\n\nYou can run this base, headless version of the application in Kaggle or other notebook systems. If you want to modify it based off of the project code, for instance to run another version, keep these points in mind:\n\n* The source code is organized into projects which have similar function names and possibly variables, so you cannot necessarily combine all functionality into one notebook.\n* The project is broken up into packages.  You will need to remove package-based imports and consider the order in which you place functionality into cells.\n * You will need to remove the standalone testing stub from the json parsing code if you reimport it.\n\nAlso note that you will need to modify the code if you place the prompts in a directory other than that specified in this code.","metadata":{}},{"cell_type":"code","source":"%pip install -q bitsandbytes transformers accelerate json5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ['INFRA_ABILITY'] = 'HIGH'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# core/app_config.py\n\"\"\"\nHandles application-wide configuration: environment detection, secrets, and hardware profiles.\nThis module is the single source of truth for the runtime environment.\n\"\"\"\n\nimport os\nimport torch\nfrom pathlib import Path\nfrom transformers import BitsAndBytesConfig\nfrom typing import Dict, Any\n\n# --- Environment Detection & Constants ---\nIS_KAGGLE_ENV: bool = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\nBASE_DIR: Path = Path(\"/kaggle/input/medgemma-laurie-prompts\") if IS_KAGGLE_ENV else Path(__file__).parent.parent.resolve()\nPROMPT_FILE_PATH: Path = BASE_DIR / \"json_prompt.txt\"\nCONVERSATIONAL_PROMPT_FILE_PATH: Path = BASE_DIR / \"conversational_prompt.txt\"\n\n# --- Secret & Environment Variable Loading ---\nif IS_KAGGLE_ENV:\n    print(\"‚úÖ Running in Kaggle environment.\")\n    try:\n        from kaggle_secrets import UserSecretsClient # type: ignore\n        user_secrets = UserSecretsClient()\n        os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = user_secrets.get_secret(\"HUGGING_FACE_HUB_TOKEN\")\n        print(\"‚úÖ Kaggle secret 'HUGGING_FACE_HUB_TOKEN' loaded into environment.\")\n    except (ImportError, Exception) as e:\n        print(f\"üî• Failed to get Kaggle secret. Model loading may fail. Error: {e}\")\nelse:\n    print(\"‚úÖ Running in a local environment.\")\n    try:\n        from dotenv import load_dotenv\n        load_dotenv()\n        print(\"‚úÖ Local .env file processed.\")\n    except ImportError:\n        print(\"‚ö†Ô∏è `python-dotenv` not found. Relying on manually set environment variables.\")\n\n# --- Core Infrastructure Configuration ---\nINFRA_ABILITY: str = os.environ.get(\"INFRA_ABILITY\", \"LOW\").upper()\n# MODEL_MODE \"VLM\" for text and image\nMODEL_MODE: str = os.environ.get(\"MODEL_MODE\", \"TEXT\").upper() # Used by LLM service\n\n# Defines hardware and performance configurations for all potential services.\nINFRA_CONFIGS: Dict[str, Dict[str, Any]] = {\n    \"LOW\": {\n        \"llm_init_args\": {\n            \"quantization_config\": BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16),\n            \"torch_dtype\": torch.bfloat16,\n            \"device_map\": \"auto\"\n        },\n        \"llm_gen_args\": {\n            \"max_new_tokens\": 1536,\n            \"do_sample\": False\n        },\n        \"asr_init_args\": {\n            \"quantization_config\": BitsAndBytesConfig(load_in_4bit=True),\n            \"torch_dtype\": torch.float16\n        }\n    },\n    \"HIGH\": {\n        \"llm_init_args\": {\n            \"quantization_config\": None,\n            \"torch_dtype\": torch.bfloat16,\n            \"device_map\": \"auto\"\n        },\n        \"llm_gen_args\": {\n            \"max_new_tokens\": 2048,\n            \"do_sample\": True,\n            \"temperature\": 0.6,\n            \"top_p\": 0.9\n        },\n        \"asr_init_args\": {\n            \"quantization_config\": None,\n            \"torch_dtype\": torch.float16\n        }\n    },\n    \"APPLE_SILICON\": {\n        \"llm_init_args\": {\n            \"quantization_config\": None,\n            \"torch_dtype\": torch.float32,\n            \"device_map\": \"mps\"\n        },\n        \"llm_gen_args\": {\n            \"max_new_tokens\": 3096,\n            \"do_sample\": True,\n            \"temperature\": 0.3,\n            \"top_p\": 0.9\n        },\n        \"asr_init_args\": {\n            \"quantization_config\": None,\n            \"torch_dtype\": torch.float32\n        }\n    }\n}\n\nactive_infra_config: Dict[str, Any] = INFRA_CONFIGS.get(INFRA_ABILITY, INFRA_CONFIGS[\"LOW\"])\nprint(f\"‚úÖ App config loaded. Mode: '{INFRA_ABILITY}'\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# services/llm/config.py\n\"\"\"LLM-specific model configurations.\"\"\"\nfrom typing import Dict, Any\n\n# --- Prompt Loading ---\ntry:\n    system_prompt_text = PROMPT_FILE_PATH.read_text()\nexcept FileNotFoundError:\n    print(f\"üî• CRITICAL: Prompt file not found at '{PROMPT_FILE_PATH}'. Using fallback.\")\n    system_prompt_text = \"You are a helpful medical assistant.\"\n\ntry:\n    conversational_prompt_text = CONVERSATIONAL_PROMPT_FILE_PATH.read_text()\nexcept FileNotFoundError:\n    print(f\"üî• CRITICAL: Prompt file not found at '{CONVERSATIONAL_PROMPT_FILE_PATH}'. Using fallback.\")\n    conversational_prompt_text = \"You are a helpful medical assistant.\"\n\n# --- LLM Model Definitions ---\nMODEL_CONFIGS: Dict[str, Dict[str, Any]] = {\n    \"TEXT\": {\n        \"model_id\": \"google/medgemma-1.5-4b-it\",\n        \"pipeline_task\": \"text-generation\",\n        \"system_prompt\": system_prompt_text,\n        \"conversational_prompt\": conversational_prompt_text\n    },\n    # --- MODIFIED: VLM configuration now uses MedGemma ---\n    \"VLM\": {\n        \"model_id\": \"google/medgemma-1.5-4b-it\", # Unified model for both text and vision\n        \"pipeline_task\": \"image-text-to-text\",        # Correct pipeline for multimodal input\n        \"system_prompt\": system_prompt_text,\n        \"conversational_prompt\": conversational_prompt_text\n    }\n}\nactive_model_config: Dict[str, Any] = MODEL_CONFIGS.get(MODEL_MODE, MODEL_CONFIGS[\"TEXT\"])\nprint(f\"‚úÖ LLM Service Mode: '{MODEL_MODE}' using model '{active_model_config['model_id']}'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# services/llm/engine.py\n\"\"\"Initializes and provides the core LLM text-generation and image-comprehension function.\"\"\"\nfrom typing import Optional, Dict, Any, Union, List\nimport torch\nfrom transformers import (\n    AutoProcessor,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    PreTrainedModel,\n    PreTrainedTokenizer,\n)\n\n# ==============================================================================\n# SETUP: Define the Generator and its Initialization Function\n# ==============================================================================\n_initialized = False\n_model = None\n_processor = None\n\n# This dictionary holds the loaded model and tokenizer\nGeneratorObjects = Dict[str, Union[PreTrainedModel, PreTrainedTokenizer]]\n\ndef initialize_generator(\n    model_id: str,\n    torch_dtype: torch.dtype,\n    quantization_config: BitsAndBytesConfig = None,\n    device_map: str = \"auto\",\n    **kwargs: Any # Catches unused arguments like 'task'\n) -> GeneratorObjects:\n    \"\"\"\n    Loads and initializes the model and tokenizer, returning them in a dictionary.\n    \"\"\"\n\n    global _initialized\n    global _model\n    global _processor\n    if _initialized:\n        return {\"model\": _model, \"processor\": _processor}\n\n    print(\"üöÄ Initializing model and processor for direct generation...\")\n    print(f\"   - Model: {model_id}\")\n    print(f\"   - DType: {torch_dtype}\")\n    print(f\"   - Quantization: {'Enabled' if quantization_config else 'Disabled'}\")\n\n    try:\n        _model = AutoModelForCausalLM.from_pretrained(\n            model_id,\n            torch_dtype=torch_dtype,\n            quantization_config=quantization_config,\n            device_map=device_map,\n        )\n        # Load a processor which can handle both text and images.\n        _processor = AutoProcessor.from_pretrained(model_id)\n        \n        # Many models like Llama don't have a pad token, so we use the EOS token.\n        if _processor.tokenizer.pad_token is None:\n            _processor.tokenizer.pad_token = _processor.tokenizer.eos_token\n        return {\"model\": _model, \"processor\": _processor}\n\n    except Exception as e:\n        print(f\"üî• CRITICAL: Model initialization failed. Error: {e}\")\n        return {}\n\n\ndef generate_text(\n    generator: GeneratorObjects,\n    messages: List[Dict[str, str]],\n    image_input: Optional[str],\n    **kwargs: Any\n) -> str:\n    \"\"\"\n    A stateless text generation function using the model.generate() method.\n    This function replaces the call to the `pipe()` object.\n    \"\"\"\n    if not generator:\n        print(\"üî• Generation failed: model and tokenizer not available.\")\n        return \"Error: Generator not initialized.\"\n\n    model = generator[\"model\"]\n    _processor = generator[\"processor\"]\n    \n    print(\"Prepare inputs with the processor template\")\n    # --- Step 1: Prepare inputs using the processor ---\n    # Mode: Use the processor's internal tokenizer to apply the chat template\n    prompt_text = _processor.tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n\n    if image_input:\n        # VLM Mode: Load image and process text/image together\n        inputs = _processor(text=prompt_text, images=image_input, return_tensors=\"pt\").to(model.device)\n    else:\n        inputs = _processor.tokenizer(prompt_text, return_tensors=\"pt\", return_attention_mask=True).to(model.device)\n\n    print(\"Generate the model outputs...\")\n    # 2. Call model.generate()\n    outputs = model.generate(**inputs, **kwargs)\n\n    # 3. Decode only the newly generated tokens, not the original prompt\n    # --- Step 3: Decode ---\n    input_ids_len = inputs[\"input_ids\"].shape[1]\n    newly_generated_ids = outputs[0, input_ids_len:]\n    generated_text = _processor.decode(newly_generated_ids, skip_special_tokens=True)\n\n    return generated_text\n\n\ndef generate_summary(prompt_key: str, medical_text: str, image_input: Optional[str] = None) -> str:\n    \"\"\"\n    Generates a summary from medical text or provides comprehension for an image.\n    - In 'TEXT' mode, it uses `medical_text`.\n    - In 'VLM' mode, it uses both `image_input` (as a file path) and `medical_text` (as a prompt).\n    \"\"\"\n    # --- Logic is split based on MODEL_MODE ---\n    if MODEL_MODE == \"VLM\":\n        if not image_input:\n            return \"Image input is required for VLM mode.\"\n        if not medical_text or not medical_text.strip():\n            return \"A text prompt is required for VLM mode.\"\n\n        try:\n            llm_generator = initialize_generator(\n                model_id=active_model_config[\"model_id\"],\n                **active_infra_config[\"llm_init_args\"]\n            )\n            # The 'image-text-to-text' pipeline requires a structured chat format.\n            # The user's message content is a list containing dicts for the image and text.\n            messages = [\n                {\n                    \"role\": \"system\",\n                    # The content is a LIST containing a DICT\n                    \"content\": [{\"type\": \"text\", \"text\": active_model_config[prompt_key]}]\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"image\", \"url\": image_input},\n                        {\"type\": \"text\", \"text\": medical_text}\n                    ]\n                }\n            ]\n\n            print(\"‚è≥ Applying chat template and generating text...\")\n            output = generate_text(\n                generator=llm_generator,\n                messages=messages,\n                image_input=image_input, # Pass the image path/URL here\n                **active_infra_config[\"llm_gen_args\"]\n            )\n            print(\"‚úÖ VLM generation complete.\")\n\n            # The output structure for image-to-text is slightly different\n            return output.strip()\n        except FileNotFoundError:\n            print(f\"üî• Error: Image file not found at '{image_input}'\")\n            return f\"Error: Image file not found at '{image_input}'\"\n        except Exception as e:\n            print(f\"üî• An error occurred during VLM model inference: {e}\")\n            return \"Sorry, an error occurred while processing your request.\"\n\n    else: # Handles 'TEXT' mode\n        if not medical_text or not medical_text.strip():\n            return \"Please enter a medical text to process.\"\n\n        messages = [\n            {\"role\": \"system\", \"content\": active_model_config[prompt_key]},\n            {\"role\": \"user\", \"content\": medical_text},\n        ]\n        try:\n            print(\"‚è≥ Generating text...\")\n\n            llm_generator = initialize_generator(\n                model_id=active_model_config[\"model_id\"],\n                **active_infra_config[\"llm_init_args\"]\n            )\n\n            result = generate_text(\n                generator=llm_generator,\n                messages=messages,\n                image_input=None,\n                **active_infra_config[\"llm_gen_args\"]\n            )\n\n            print(\"‚úÖ Text generation complete.\")\n            return result\n        except Exception as e:\n            print(f\"üî• An error occurred during text model inference: {e}\")\n            return \"Sorry, an error occurred while processing your request.\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# core_logic.py\n\nimport json5\nimport re\n\ndef strip_xml_tags(text: str) -> str:\n    \"\"\"\n    If the text is wrapped in a single XML-like tag, strips the tag.\n    Otherwise, returns the original text.\n    Handles tags like <answer>...</answer> or <response>...</response>.\n    \"\"\"\n    # This pattern looks for <tag>content</tag> and captures the content.\n    match = re.search(r\"<(?P<tag>\\w+)>(?P<content>.*?)</(?P=tag)>\", text.strip(), re.DOTALL)\n    \n    if match:\n        # If a match is found, return the captured content, stripped of whitespace.\n        return match.group(\"content\").strip()\n    \n    # If no tags are found, return the original text.\n    return text.strip()\n\ndef get_llm_summary(text,image_path):\n    \"\"\"\n    This function encapsulates the entire long-running task.\n    It is executed in the background thread by the animator.\n    It must RETURN a final string, not yield.\n    \"\"\"\n    # 1. Call the long-running AI function\n    summary_result = generate_summary(\"system_prompt\", text, image_path)\n    # print(\"Summary result: \", summary_result)\n\n    # 2. Clean and parse the result\n    json_str = extract_json_from_text(summary_result)\n    if not json_str:\n        # Return an error string for the UI\n        # return \"Error: The AI engine did not return a valid summary structure. Please try again.\"\n        # Better to return a poorly formatted string, rather than just an error message...\n        return summary_result\n\n    json_data = json5.loads(json_str)\n    \n    # 3. Format the JSON into readable text\n    formatted_text = format_medical_info(json_data)\n    \n    # 4. Return the final, display-ready text\n    return formatted_text\n\n\ndef get_follow_up_answer(original_text, summary, history, new_question, image_path):\n    \"\"\"\n    (New Function) Generates an answer to a follow-up question.\n    \n    This function constructs a detailed prompt that gives the LLM all necessary\n    context to provide a relevant, grounded answer.\n    \"\"\"\n    # Construct a history string from the conversation\n    history_str = \"\\n\".join([f\"User: {q}\\nAI: {a}\" for q, a in history])\n\n    # Construct the full prompt for the LLM\n    chat_text = f\"\"\"\n\n<medical_text>\n{original_text}\n</medical_text>\n\n<summary_of_text>\n{summary}\n</summary_of_text>\n\n<conversation_history>\n{history_str}\n</conversation_history>\n\n<user_question>\n{new_question}\n</user_question>\n\nProvide your answer directly.\n\"\"\"\n    # Make the call to the LLM. This is expected to be a faster, simpler call.\n    raw_answer = generate_summary(\"conversational_prompt\", chat_text, image_path) \n    \n    # Sometimes the LLM likes to resond with XML...\n    clean_answer = strip_xml_tags(raw_answer)\n\n    return clean_answer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# core/parse_json.py\n\nimport json5\nimport re\nimport html\nfrom typing import Dict, List, Any, Optional\n\n\ndef extract_json_from_text(raw_text: str) -> Optional[str]:\n    \"\"\"\n    Extract JSON from messy LLM output with multiple strategies.\n    Returns the last valid JSON object found, or None.\n    \"\"\"\n    text = raw_text.strip()\n\n    # Strategy 1: Look for ```json blocks and try the last one\n    json_blocks = re.findall(r'```json\\s*(\\{.*?\\})\\s*```', text, re.DOTALL)\n    if json_blocks:\n        for block in reversed(json_blocks):\n            try:\n                json5.loads(block)\n                return block\n            except ValueError:\n                continue\n    \n    # Strategy 2: Find all complete {...} blocks and try the last valid one\n    potential_jsons = []\n    depth = 0\n    start = -1\n    \n    for i, char in enumerate(text):\n        if char == '{':\n            if depth == 0:\n                start = i\n            depth += 1\n        elif char == '}':\n            depth -= 1\n            if depth == 0 and start != -1:\n                potential_jsons.append(text[start:i+1])\n                start = -1\n    \n    for candidate in reversed(potential_jsons):\n        try:\n            json5.loads(candidate)\n            return candidate\n        except ValueError:\n            continue\n    \n    return None\n\n\ndef safe_get_list(data: Dict, key: str) -> List[str]:\n    \"\"\"Safely extract a list of strings, handling errors and duplicates.\"\"\"\n    value = data.get(key)\n    if value is None:\n        return []\n    if not isinstance(value, list):\n        value = [value]\n    \n    seen = set()\n    result = []\n    for item in value:\n        item_str = html.escape(str(item).strip()) if item is not None else \"\"\n        if item_str and item_str not in seen:\n            seen.add(item_str)\n            result.append(item_str)\n    return result\n\n\ndef safe_get_dict_list(data: Dict, key: str) -> List[Dict]:\n    \"\"\"Safely extract list of dictionaries.\"\"\"\n    value = data.get(key)\n    if value is None:\n        return []\n    if isinstance(value, dict):\n        value = [value]\n    if not isinstance(value, list):\n        return []\n    return [item for item in value if isinstance(item, dict)]\n\n\ndef safe_get_string(data: Dict, key: str, default: str = '') -> str:\n    \"\"\"Safely get string value and escape for HTML safety.\"\"\"\n    value = data.get(key)\n    if value is None or value == '':\n        return default\n    return html.escape(str(value).strip())\n\n\ndef format_medical_info(json_data: Dict[str, Any]) -> str:\n    \"\"\"\n    Formats the structured medical JSON using semantic HTML tags \n    that Quill's clipboard parser recognizes and maps to its toolbar.\n    \"\"\"\n    output_parts = []\n    \n    # 1. Header & Disclaimer\n    output_parts.append(\"<h2>Medical Summary</h2>\")\n    output_parts.append(\"<blockquote><b>‚ö†Ô∏è DISCLAIMER:</b> This is not medical advice. Please consult your medical professional.</blockquote>\")\n\n    # 2. Key Takeaways (Bullet List)\n    takeaways = safe_get_list(json_data, 'key_takeaways')\n    if takeaways:\n        output_parts.append(\"<h3>üìå Key Takeaways</h3><ul>\")\n        for item in takeaways:\n            output_parts.append(f\"<li>{item}</li>\")\n        output_parts.append(\"</ul>\")\n\n    # 3. Medications \n    medications = safe_get_dict_list(json_data, 'medications')\n    if medications:\n        output_parts.append(\"<h3>üíä Medications</h3>\")\n        for med in medications:\n            name = safe_get_string(med, 'name', 'Unknown')\n            dosage = safe_get_string(med, 'dosage')\n            admin = safe_get_string(med, 'administration')\n            desc = safe_get_string(med, 'description')\n            \n            # Use single-level paragraphs with indentation or bullets\n            output_parts.append(f\"<p><b>‚Ä¢ {name}</b></p>\")\n            if dosage: output_parts.append(f\"<p style='margin-left: 20px;'>- Dosage: {dosage}</p>\")\n            if admin:  output_parts.append(f\"<p style='margin-left: 20px;'>- How to take: {admin}</p>\")\n            if desc:   output_parts.append(f\"<p style='margin-left: 20px;'><i>{desc}</i></p>\")\n        output_parts.append(\"<p><br></p>\")\n\n    # 4. Terms (Bold and Inline)\n    terms = safe_get_dict_list(json_data, 'medical_terms')\n    if terms:\n        output_parts.append(\"<h3>üìñ Terms Explained</h3>\")\n        for term_obj in terms:\n            term = safe_get_string(term_obj, 'term', 'Unknown')\n            defn = safe_get_string(term_obj, 'definition', 'N/A')\n            output_parts.append(f\"<p><b>{term}</b>: {defn}</p>\")\n        output_parts.append(\"<p><br></p>\")\n\n    # 5. Questions (Ordered List)\n    questions = safe_get_list(json_data, 'questions_for_provider')\n    if questions:\n        output_parts.append(\"<h3>‚ùì Questions for Provider</h3><ol>\")\n        for q in questions:\n            output_parts.append(f\"<li>{q}</li>\")\n        output_parts.append(\"</ol>\")\n        output_parts.append(\"<p><br></p>\")\n    \n    return \"\".join(output_parts)\n\n\ndef format_medical_info_from_string(raw_text: str) -> str:\n    \"\"\"Extracts JSON and converts to HTML directly from LLM string.\"\"\"\n    json_str = extract_json_from_text(raw_text)\n    if not json_str:\n        return \"<p>Could not find valid JSON data.</p>\"\n    \n    try:\n        json_data = json5.loads(json_str)\n        return format_medical_info(json_data)\n    except Exception as e:\n        return f\"<p>Error parsing JSON: {html.escape(str(e))}</p>\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# A test of the system\n\nNote that the output will be a (hopefully valid) JSON string","metadata":{}},{"cell_type":"code","source":"# 1. Define your input text\nmedical_text = \"\"\"\nThe patient is a 68-year-old Korean gentleman with a history of coronary artery disease, hypertension, diabetes and stage III CKD with a creatinine of 1.8 in May 2006 corresponding with the GFR of 40-41 mL/min. The patient had blood work done at Dr. XYZ's office on June 01, 2006, which revealed an elevation in his creatinine up to 2.3. He was asked to come in to see a nephrologist for further evaluation. I am therefore asked by Dr. XYZ to see this patient in consultation for evaluation of acute on chronic kidney failure. The patient states that he was actually taking up to 12 to 13 pills of Chinese herbs and dietary supplements for the past year. He only stopped about two or three weeks ago. He also states that TriCor was added about one or two months ago but he is not sure of the date. He has not had an ultrasound but has been diagnosed with prostatic hypertrophy by his primary care doctor and placed on Flomax. He states that his urinary dribbling and weak stream had not improved since doing this. For the past couple of weeks, he has had dizziness in the morning. This is then associated with low glucose. However the patient's blood glucose this morning was 123 and he still was dizzy. This was worse on standing. He states that he has been checking his blood pressure regularly at home because he has felt so bad and that he has gotten under 100/60 on several occasions. His pulses remained in the 60s.\n\"\"\"\n\n# 2. Call the generation function\nprint(\"\\n--- Calling get_llm_summary ---\")\nsummary = get_llm_summary(medical_text, None)\n\n# 3. Print the final result\nprint(\"\\n--- Final Generated Summary ---\")\nprint(summary)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I don't have the link to the sample images on this computer or in the medgemma notes, so I can't actually test the image work while travelling...","metadata":{}},{"cell_type":"markdown","source":"# A Second test","metadata":{}},{"cell_type":"code","source":"# 1. Define your input text\n\nmedical_text = \"\"\"\nOne day history upper abdominal pain. Elevated inflammatory markers. On physical exam, right sided abdominal tenderness with guarding in right iliac fossa ?appendicitis.\n\nHistopathology report.\nBiopsies from around lower pole of left kidney:\nCores of fibrovascular and adipose tissue including a small amount of inflamed renal cortical tissue at limit of one core.\nExtensive fibrosis and focal fat necrosis; in areas the background is edematous with moderate chronic inflammation including few lymphoid aggregates and clusters of plasma cells with scattered mast cells. Spindle cell proliferation of bland cells as well as scattered foamy histiocytes. No evidence of malignancy. Immunostaining showing that the spindle cells are histiocytes: CD68 and CD163 positive with strong expression of factor XIIIa and focal staining for S100. Staining for CD1a and Braf is essentially negative.\nAbridged genetics report: M117.1, BRAF c.1799T>A p.(Val600Glu) V600E pathogenic hotspot variant detected manually at very low level (3.5% VAF).\nFindings consistent with Erdheim Chester disease.\nCase Discussion\nThe renal findings in this case are distinctive and indeed virtually pathognomonic of Erdheim-Chester disease. Nevertheless it is useful to bear in mind mimics of the appearances if they are less florid such as lymphoma.\n\"\"\"\n\n# 2. Call the generation function\nprint(\"\\n--- Calling get_llm_summary ---\")\nsummary = get_llm_summary(medical_text, None)\n\n# 3. Print the final result\nprint(\"\\n--- Final Generated Summary ---\")\nprint(summary)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}